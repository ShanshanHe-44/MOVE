{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "from typing import Literal, Sized, Union, cast\n",
    "\n",
    "import hydra\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from move.conf.schema import (\n",
    "    IdentifyAssociationsBayesConfig,\n",
    "    IdentifyAssociationsConfig,\n",
    "    IdentifyAssociationsTTestConfig,\n",
    "    MOVEConfig,\n",
    ")\n",
    "from move.core.logging import get_logger\n",
    "from move.core.typing import FloatArray, IntArray\n",
    "from move.data import io\n",
    "from move.data.dataloaders import MOVEDataset, make_dataloader\n",
    "from move.data.perturbations import perturb_categorical_data\n",
    "from move.data.preprocessing import one_hot_encode_single\n",
    "from move.models.vae import VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/wkq953/Desktop/CPR/4. codes/move/tutorial/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/wkq953/Desktop/CPR/4. codes/move/tutorial\n"
     ]
    }
   ],
   "source": [
    "cd .. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initiate MOVE with test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from move.data import io\n",
    "from move.tasks import encode_data\n",
    "import numpy as np\n",
    "#config = io.read_config(\"random_small\", \"encode_data\")\n",
    "#config = io.read_config(\"random_continuous\", \"encode_data\")\n",
    "#config = io.read_config(\"random_catagorical\", \"encode_data\")\n",
    "config = io.read_config(\"random_test\", \"encode_data\")\n",
    "\n",
    "encode_data(config.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random.test.drugs: (500, 5, 2)\n",
      "random.test.metagenomics: (500, 20, 2)\n",
      "random.test.proteomics: (500, 20)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "path = Path(config.data.interim_data_path)\n",
    "cat_datasets, cat_names, con_datasets, con_names = io.load_preprocessed_data(path, config.data.categorical_names, config.data.continuous_names)\n",
    "cat_names\n",
    "dataset_names = config.data.categorical_names + config.data.continuous_names\n",
    "for dataset, dataset_name in zip(cat_datasets + con_datasets, dataset_names):\n",
    "    print(f\"{dataset_name}: {dataset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune model: no need to run currently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seed': None, 'data': {'raw_data_path': 'data/test', 'interim_data_path': 'test_data/interim_data/', 'results_path': 'test_data/results/', 'sample_names': 'random.test.ids', 'categorical_inputs': [{'name': 'random.test.drugs'}, {'name': 'random.test.metagenomics'}], 'continuous_inputs': [{'name': 'random.test.proteomics'}], 'categorical_names': '${names:${data.categorical_inputs}}', 'continuous_names': '${names:${data.continuous_inputs}}', 'categorical_weights': '${weights:${data.categorical_inputs}}', 'continuous_weights': '${weights:${data.continuous_inputs}}'}, 'task': {'batch_size': 10, 'model': {'_target_': 'move.models.vae.VAE', 'cuda': False, 'categorical_weights': '${weights:${data.categorical_inputs}}', 'continuous_weights': '${weights:${data.continuous_inputs}}', 'num_hidden': [1000], 'num_latent': 150, 'beta': 0.0001, 'dropout': 0.1}, 'training_loop': {'_target_': 'move.training.training_loop.training_loop', 'num_epochs': 40, 'lr': 0.0001, 'kld_warmup_steps': [15, 20, 25], 'batch_dilation_steps': [50, 100, 150], 'early_stopping': False, 'patience': 0}, 'num_refits': 3}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## another way to initial the model \n",
    "config = io.read_config(\"random_test\", \"tune_model_reconstruction\")\n",
    "config = io.read_config(\"random_test\", \"tune_model_stability\")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! move-dl experiment=random_test__tune_reconstruction\n",
    "! move-dl experiment=random_test__tune_stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "results = pd.read_csv(\"test_data/results/tune_model/reconstruction_stats.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent space analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO  - analyze_latent]: Beginning task: analyze latent space\n",
      "[INFO  - analyze_latent]: Projecting into latent space\n",
      "[INFO  - analyze_latent]: Reconstructing\n",
      "[INFO  - analyze_latent]: Computing reconstruction metrics\n",
      "[INFO  - analyze_latent]: Computing feature importance\n"
     ]
    }
   ],
   "source": [
    "from move.tasks import analyze_latent\n",
    "#config = io.read_config(\"random_catagorical\", \"random_catagorical__latent\")\n",
    "config = io.read_config(\"random_test\", \"random_test__latent\")\n",
    "#config = io.read_config(\"random_continuous\", \"random_continuous__latent\")\n",
    "#config = io.read_config(\"random_small\", \"random_small__latent\")\n",
    "#print(OmegaConf.to_yaml(config, resolve=True))\n",
    "\n",
    "analyze_latent(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Association test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from move.tasks import identify_associations\n",
    "# config = io.read_config(\"random_small\", \"random_small__id_assoc_bayes\")\n",
    "config = io.read_config(\"random_test\", \"random_test__id_assoc_bayes\")\n",
    "#identify_associations(config)\n",
    "#identify_associations_cat(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seed': None, 'data': {'raw_data_path': 'data/', 'interim_data_path': 'interim_data_test/', 'results_path': 'results_test/', 'sample_names': 'random.test.ids', 'categorical_inputs': [{'name': 'random.test.drugs'}, {'name': 'random.test.metagenomics'}], 'continuous_inputs': [{'name': 'random.test.proteomics'}], 'categorical_names': '${names:${data.categorical_inputs}}', 'continuous_names': '${names:${data.continuous_inputs}}', 'categorical_weights': '${weights:${data.categorical_inputs}}', 'continuous_weights': '${weights:${data.continuous_inputs}}'}, 'task': {'batch_size': 10, 'model': {'_target_': 'move.models.vae.VAE', 'cuda': False, 'categorical_weights': '${weights:${data.categorical_inputs}}', 'continuous_weights': '${weights:${data.continuous_inputs}}', 'num_hidden': [100], 'num_latent': 10, 'beta': 0.0001, 'dropout': 0.1}, 'training_loop': {'_target_': 'move.training.training_loop.training_loop', 'num_epochs': 40, 'lr': 0.0001, 'kld_warmup_steps': [15, 20, 25], 'batch_dilation_steps': [50, 100, 150], 'early_stopping': False, 'patience': 0}, 'target_dataset': 'random.test.drugs', 'target_value': '1', 'num_refits': 10, 'sig_threshold': 0.05, 'save_refits': True}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deep into the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import hydra\n",
    "from move.data import io\n",
    "from move.data.dataloaders import make_dataloader\n",
    "from move.data.perturbations import perturb_categorical_data\n",
    "\n",
    "cfg = io.read_config(\"random_test\", \"random_test__id_assoc_bayes\")\n",
    "\n",
    "cat_list, cat_names, con_list, con_names = io.load_preprocessed_data(\n",
    "    Path(cfg.data.interim_data_path),\n",
    "    cfg.data.categorical_names,\n",
    "    cfg.data.continuous_names,\n",
    ")\n",
    "\n",
    "baseline_dataloader = make_dataloader(\n",
    "    cat_list,\n",
    "    con_list,\n",
    "    shuffle=False,\n",
    "    batch_size=cfg.task.batch_size,\n",
    ")\n",
    "baseline_dataset = baseline_dataloader.dataset\n",
    "\n",
    "dataloaders = perturb_categorical_data(\n",
    "    baseline_dataloader,\n",
    "    cfg.data.categorical_names,\n",
    "    \"random.test.drugs\",\n",
    "    np.array([0, 1]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE (70 ⇄ 12 ⇄ 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = hydra.utils.instantiate(\n",
    "    cfg.task.model,\n",
    "    continuous_shapes=baseline_dataset.con_shapes,\n",
    "    categorical_shapes=baseline_dataset.cat_shapes,\n",
    ")\n",
    "# require pre-trained model\n",
    "model.load_state_dict(torch.load(\"results_test/latent_space/model.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How to get the NA mask from original catagorical dataset since drug and meta are combine together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
       "        0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_cat = baseline_dataset.cat_all # 2D N x Cat (500,50) 5*2+20*2\n",
    "#orig_cat = orig_cat[-1] # remove drug data\n",
    "orig_cat[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. how to deal with refit number \n",
    "\n",
    "probability / num_refit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. cat shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = [\"identify_associations\"]\n",
    "\n",
    "from functools import reduce\n",
    "from os.path import exists\n",
    "from pathlib import Path\n",
    "from typing import Literal, Sized, Union, cast\n",
    "\n",
    "import hydra\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from scipy.stats import ks_2samp, pearsonr  # type: ignore\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from move.analysis.metrics import get_2nd_order_polynomial\n",
    "\n",
    "from move.conf.schema import (\n",
    "    IdentifyAssociationsBayesConfig,\n",
    "    IdentifyAssociationsBayesCatConfig,\n",
    "    IdentifyAssociationsConfig,\n",
    "    IdentifyAssociationsKSConfig,\n",
    "    IdentifyAssociationsTTestConfig,\n",
    "    MOVEConfig,\n",
    ")\n",
    "from move.core.logging import get_logger\n",
    "from move.core.typing import BoolArray, FloatArray, IntArray\n",
    "from move.data import io\n",
    "from move.data.dataloaders import MOVEDataset, make_dataloader\n",
    "from move.data.perturbations import (\n",
    "    ContinuousPerturbationType,\n",
    "    perturb_categorical_data,\n",
    "    perturb_continuous_data_extended,\n",
    ")\n",
    "from move.data.preprocessing import one_hot_encode_single\n",
    "from move.models.vae import VAE\n",
    "from move.visualization.dataset_distributions import (\n",
    "    plot_correlations,\n",
    "    plot_cumulative_distributions,\n",
    "    plot_feature_association_graph,\n",
    "    plot_reconstruction_movement,\n",
    ")\n",
    "\n",
    "TaskType = Literal[\"bayes\", \"bayes_cat\",\"ttest\", \"ks\"]\n",
    "CONTINUOUS_TARGET_VALUE = [\"minimum\", \"maximum\", \"plus_std\", \"minus_std\"]\n",
    "\n",
    "def prepare_for_categorical_perturbation(\n",
    "    config: MOVEConfig,\n",
    "    interim_path: Path,\n",
    "    baseline_dataloader: DataLoader,\n",
    "    cat_list: list[FloatArray],\n",
    ") -> tuple[list[DataLoader], BoolArray, BoolArray,]:\n",
    "    \"\"\"\n",
    "    This function creates the required dataloaders and masks\n",
    "    for further categorical association analysis.\n",
    "\n",
    "    Args:\n",
    "        config: main configuration file\n",
    "        interim_path: path where the intermediate outputs are saved\n",
    "        baseline_dataloader: reference dataloader that will be perturbed\n",
    "        cat_list: list of arrays with categorical data\n",
    "\n",
    "    Returns:\n",
    "        dataloaders: all dataloaders, including baseline appended last.\n",
    "        nan_mask: mask for Nans\n",
    "        feature_mask: masks the column for the perturbed feature.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read original data and create perturbed datasets\n",
    "    task_config = cast(IdentifyAssociationsConfig, config.task)\n",
    "    logger = get_logger(__name__)\n",
    "\n",
    "    # Loading mappings:\n",
    "    mappings = io.load_mappings(interim_path / \"mappings.json\")\n",
    "    target_mapping = mappings[task_config.target_dataset]\n",
    "    target_value = one_hot_encode_single(target_mapping, task_config.target_value)\n",
    "    logger.debug(\n",
    "        f\"Target value: {task_config.target_value} => {target_value.astype(int)[0]}\"\n",
    "    )\n",
    "\n",
    "    dataloaders = perturb_categorical_data(\n",
    "        baseline_dataloader,\n",
    "        config.data.categorical_names,\n",
    "        task_config.target_dataset,\n",
    "        target_value,\n",
    "    )\n",
    "    dataloaders.append(baseline_dataloader)\n",
    "\n",
    "    \n",
    "    baseline_dataset = cast(MOVEDataset, baseline_dataloader.dataset)\n",
    "\n",
    "    assert baseline_dataset.con_all is not None\n",
    "    orig_con = baseline_dataset.con_all\n",
    "    nan_mask = (orig_con == 0).numpy()  # NaN values encoded as 0s\n",
    "    logger.debug(f\"# NaN values: {np.sum(nan_mask)}/{orig_con.numel()}\")\n",
    "\n",
    "    target_dataset_idx = config.data.categorical_names.index(task_config.target_dataset)\n",
    "    target_dataset = cat_list[target_dataset_idx]\n",
    "    feature_mask = np.all(target_dataset == target_value, axis=2)  # 2D: N x P\n",
    "    feature_mask |= np.sum(target_dataset, axis=2) == 0\n",
    "    cat_list_wo_target = cat_list.copy()\n",
    "    cat_list_wo_target.pop(target_dataset_idx)\n",
    "    nan_mask_cat = [np.all(i == [0,0], axis = 2) for i in cat_list_wo_target] # in case there are more than one catagorical dataset\n",
    "    nan_mask_cat = np.hstack(nan_mask_cat)\n",
    "\n",
    "    return (\n",
    "        dataloaders,\n",
    "        nan_mask,\n",
    "        feature_mask,\n",
    "        nan_mask_cat\n",
    "    )\n",
    "\n",
    "def _bayes_cat_approach(\n",
    "    config: MOVEConfig,\n",
    "    task_config: IdentifyAssociationsBayesCatConfig,\n",
    "    train_dataloader: DataLoader,\n",
    "    baseline_dataloader: DataLoader,\n",
    "    dataloaders: list[DataLoader],\n",
    "    models_path: Path,\n",
    "    num_perturbed: int,\n",
    "    num_samples: int,\n",
    "    num_catagorical: int,\n",
    "    nan_mask_cat: BoolArray,\n",
    "    feature_mask: BoolArray,\n",
    ") -> tuple[Union[IntArray, FloatArray], ...]:\n",
    "\n",
    "    assert task_config.model is not None\n",
    "    device = torch.device(\"cuda\" if task_config.model.cuda == True else \"cpu\")\n",
    "\n",
    "    # Train models\n",
    "    logger = get_logger(__name__)\n",
    "    logger.info(\"Training models\")\n",
    "    mean_prob = np.zeros((num_perturbed, num_catagorical))\n",
    "    normalizer = 1 / task_config.num_refits\n",
    "    target_dataset_idx = config.data.categorical_names.index(task_config.target_dataset)\n",
    "\n",
    "    # Last appended dataloader is the baseline\n",
    "    baseline_dataset = cast(MOVEDataset, baseline_dataloader.dataset)\n",
    "\n",
    "    for j in range(task_config.num_refits):\n",
    "        # Initialize model\n",
    "        model: VAE = hydra.utils.instantiate(\n",
    "            task_config.model,\n",
    "            continuous_shapes=baseline_dataset.con_shapes,\n",
    "            categorical_shapes=baseline_dataset.cat_shapes,\n",
    "        )\n",
    "        if j == 0:\n",
    "            logger.debug(f\"Model: {model}\")\n",
    "\n",
    "        # Train/reload model\n",
    "        model_path = models_path / f\"model_{task_config.model.num_latent}_{j}.pt\"\n",
    "        if model_path.exists():\n",
    "            logger.debug(f\"Re-loading refit {j + 1}/{task_config.num_refits}\")\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            model.to(device)\n",
    "        else:\n",
    "            logger.debug(f\"Training refit {j + 1}/{task_config.num_refits}\")\n",
    "            model.to(device)\n",
    "            hydra.utils.call(\n",
    "                task_config.training_loop,\n",
    "                model=model,\n",
    "                train_dataloader=train_dataloader,\n",
    "            )\n",
    "            if task_config.save_refits:\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "        model.eval()\n",
    "\n",
    "        # Calculate baseline reconstruction\n",
    "        baseline_recat, _ = model.reconstruct(baseline_dataloader) # including drug data\n",
    "        baseline_recat.pop(target_dataset_idx)\n",
    "        baseline_recat = np.hstack(baseline_recat) # combine all catagoical data without drug data into one array\n",
    "\n",
    "\n",
    "        # Calculate perturb reconstruction => keep track of mean difference\n",
    "        for i in range(num_perturbed):\n",
    "            perturb_recat, _ = model.reconstruct(dataloaders[i])\n",
    "            perturb_recat.pop(target_dataset_idx) \n",
    "            perturb_recat = np.hstack(perturb_recat)\n",
    "            diff_recat = (perturb_recat != baseline_recat) # T: if the class change \n",
    "            mask_cat = feature_mask[:, [i]]| nan_mask_cat\n",
    "            diff_recat = np.ma.masked_array(diff_recat, mask = mask_cat)\n",
    "            prob = np.ma.compressed(np.mean(diff_recat, axis=0))  # 1D: C\n",
    "            mean_prob[i, :] = prob * normalizer\n",
    "\n",
    "\n",
    "    # Calculate Bayes factors\n",
    "    logger.info(\"Identifying significant features\")\n",
    "    bayes_k = np.empty((num_perturbed, num_catagorical))\n",
    "    bayes_mask = np.zeros(np.shape(bayes_k))\n",
    "    for i in range(num_perturbed):\n",
    "        bayes_k[i, :] = np.log(mean_prob[i,] + 1e-8) - np.log(1 - mean_prob[i,] + 1e-8)\n",
    "\n",
    "    bayes_mask[bayes_mask != 0] = 1\n",
    "    bayes_mask = np.array(bayes_mask, dtype=bool)\n",
    "\n",
    "    # Calculate Bayes probabilities\n",
    "    bayes_abs = np.abs(bayes_k)\n",
    "    bayes_p = np.exp(bayes_abs) / (1 + np.exp(bayes_abs))  # 2D: N x C\n",
    "    bayes_abs[bayes_mask] = np.min(\n",
    "        bayes_abs\n",
    "    )  # Bring feature_i feature_i associations to minimum\n",
    "    sort_ids = np.argsort(bayes_abs, axis=None)[::-1]  # 1D: N x C\n",
    "    prob = np.take(bayes_p, sort_ids)  # 1D: N x C\n",
    "    logger.debug(f\"Bayes proba range: [{prob[-1]:.3f} {prob[0]:.3f}]\")\n",
    "\n",
    "    # Sort Bayes\n",
    "    bayes_k = np.take(bayes_k, sort_ids)  # 1D: N x C\n",
    "\n",
    "    # Calculate FDR\n",
    "    fdr = np.cumsum(1 - prob) / np.arange(1, prob.size + 1)  # 1D\n",
    "    idx = np.argmin(np.abs(fdr - task_config.sig_threshold))\n",
    "    logger.debug(f\"FDR range: [{fdr[0]:.3f} {fdr[-1]:.3f}]\")\n",
    "\n",
    "    return sort_ids[:idx], prob[:idx], fdr[:idx], bayes_k[:idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = io.read_config(\"random_test\", \"random_test__id_assoc_bayes\")\n",
    "logger = get_logger(__name__)\n",
    "task_config = cast(IdentifyAssociationsConfig, config.task)\n",
    "interim_path = Path(config.data.interim_data_path)\n",
    "\n",
    "models_path = interim_path / \"models\"\n",
    "if task_config.save_refits:\n",
    "    models_path.mkdir(exist_ok=True)\n",
    "\n",
    "output_path = Path(config.data.results_path) / \"identify_associations\"\n",
    "output_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Load datasets:\n",
    "cat_list, cat_names, con_list, con_names = io.load_preprocessed_data(\n",
    "    interim_path,\n",
    "    config.data.categorical_names,\n",
    "    config.data.continuous_names,\n",
    ")\n",
    "\n",
    "train_dataloader = make_dataloader(\n",
    "    cat_list,\n",
    "    con_list,\n",
    "    shuffle=True,\n",
    "    batch_size=task_config.batch_size,\n",
    "    drop_last=True,\n",
    ")\n",
    "con_shapes = [con.shape[1] for con in con_list]\n",
    "target_dataset_idx = config.data.categorical_names.index(task_config.target_dataset)\n",
    "\n",
    "if len(cat_list) >1:\n",
    "    cat_shapes = [cat.shape[1] for cat in cat_list] # [5,20] [20]\n",
    "    cat_shapes.pop(target_dataset_idx)\n",
    "    num_catagorical = sum(cat_shapes)  # C\n",
    "\n",
    "num_samples = len(cast(Sized, train_dataloader.sampler))  # N\n",
    "num_continuous = sum(con_shapes)  # C\n",
    "\n",
    "logger.debug(f\"# continuous features: {num_continuous}\")\n",
    "\n",
    "# Creating the baseline dataloader:\n",
    "baseline_dataloader = make_dataloader(\n",
    "    cat_list, con_list, shuffle=False, batch_size=task_config.batch_size\n",
    ")\n",
    "\n",
    "(dataloaders, nan_mask, feature_mask, nan_mask_cat) = prepare_for_categorical_perturbation(\n",
    "    config, interim_path, baseline_dataloader, cat_list)\n",
    "\n",
    "num_perturbed = len(dataloaders) - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO  - __main__]: Training models\n",
      "[INFO  - __main__]: Identifying significant features\n"
     ]
    }
   ],
   "source": [
    "sig_ids, *extra_cols = _bayes_cat_approach(\n",
    "    config,\n",
    "    task_config,\n",
    "    train_dataloader,\n",
    "    baseline_dataloader,\n",
    "    dataloaders,\n",
    "    models_path,\n",
    "    num_perturbed,\n",
    "    num_samples,\n",
    "    num_catagorical,\n",
    "    nan_mask_cat,\n",
    "    feature_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mask\n",
    "## 1. drug mask \n",
    "feature_mask = np.all(cat_list[0] == [0, 1], axis=2) # (500,5) mask the samples with drug value = 1\n",
    "feature_mask |= np.sum(cat_list[0], axis=2) == 0 # (500,5) mask the samples with drug value = NA [0,0]\n",
    "## 2. meta mask\n",
    "nan_mask_cat = np.all(cat_list[-1] == [0,0], axis = 2 )  # remove NA values encoded as [0,0]\n",
    "## 3. protein mask\n",
    "orig_con = baseline_dataset.con_all # 2D: N x C\n",
    "nan_mask = (orig_con == 0).numpy()  # NaN values encoded as 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_shapes = [cat.shape[1] for cat in cat_list]\n",
    "cat_shapes = cat_shapes[1:]\n",
    "cat_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.28271928,  0.62371866,  0.47692406,  0.64250343,  0.94024395,\n",
       "        0.2135741 ,  1.64561752,  0.64250343,  0.69953696,  1.2582424 ,\n",
       "        0.31753503,  1.16203409,  1.07044139,  0.66139847,  0.23080642,\n",
       "        0.56798403,  0.37012573,  0.58646303, -0.30010459,  1.04818141])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test in one drug\n",
    "drug_idx = 0 # for i in range(num_perturbed):\n",
    "\n",
    "baseline_recat, baseline_recon = model.reconstruct(baseline_dataloader)\n",
    "perturb_recat, perturb_recon = model.reconstruct(dataloaders[drug_idx])\n",
    "diff_recon = (perturb_recon - baseline_recon) \n",
    "\n",
    "baseline_recat = baseline_recat[-1] # remove drug data \n",
    "perturb_recat = perturb_recat[-1] # remove drug data\n",
    "diff_recat = (perturb_recat == baseline_recat) # (500,20)\n",
    "normalizer = 1 / task_config.num_refits # 1/\n",
    "mask_cat = feature_mask[:, [drug_idx]]| nan_mask_cat\n",
    "diff_recat = np.ma.masked_array(diff_recat, mask = mask_cat)  # 2D: N x C  only consider the samples with drug_i from 0 to 1 \n",
    "prob = np.ma.compressed(np.mean(diff_recat, axis=0))  \n",
    "mean_pro = prob * normalizer\n",
    "bayes_k = np.log(prob + 1e-8) - np.log(1 - prob + 1e-8)\n",
    "bayes_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_diff = np.zeros((5, 500, 20))\n",
    "mean_diff[0, :, :] += diff_recat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 500, 20)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_diff.shape\n",
    "mean_diff[0, :, :] += diff_recat\n",
    "mean_diff[0, :, :] += diff_recat\n",
    "\n",
    "#mean_diff[0, :, :] = np.append (mean_diff[0, :, :] ,diff_recat)\n",
    "mean_diff.shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "move",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
